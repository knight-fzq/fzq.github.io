#!/usr/bin/env python
# coding: utf-8

# In[59]:


#构造数据,三个自变量x0=1,x1,x2,x3
import numpy as np
import matplotlib.pyplot as plt
x=np.array([[1,1.4,1.2,1],[1,2,2,1.7],[1,0.6,0.5,0.4],[1,2.4,2.8,2.7],[1,3.2,4,3]])
y=np.array([[1],[1],[1],[0],[0]])
theta=np.array([[0],[0],[0],[0]])
#学习率alpha,迭代次数iteration,代价函数值J，h是htheta(x),都是以向量形式表示和运算和输出
alpha=0.002
iteration=100000
J=np.zeros((1,iteration))
m=5
for i in range(iteration):
    h=1/(1+np.exp(-x.dot(theta)))
    J[0][i]=-1/m*np.sum(y*np.log(h)+(1-y)*np.log(1-h))
    #维度对应好，严谨的数学推导
    theta=theta-alpha*(1/m)*(x.T.dot(h-y))
x_J=np.arange(iteration)
#把列表变成数组，使维度一致
x_J=x_J[np.newaxis,:]
#绘制代价函数随迭代次数的图像，以判断是否收敛
plt.scatter(x_J,J)
plt.show()
print(theta)
#代价函数随迭代次数的图像上已经收敛，theta依次为[[ 6.49867775]，[ 2.90578187]，[-2.47306215]，[-3.28534966]]



################################################################
#进阶:封装成函数形式
def min_logistic(x,y,iteration=10000,alpha=0.002):
    J=np.zeros((1,iteration))
    m=x.shape[0][0]
    n=x.shape[0][1]
    theta=np.zeros((n,1))
    h=1/(1+np.exp(-x.dot(theta)))
    J[0][i]=-1/m*np.sum(y*np.log(h)+(1-y)*np.log(1-h))
    theta=theta-alpha*(1/m)*(x.T.dot(h-y))
    return(J,theta)


# In[ ]:




