#!/usr/bin/env python
# coding: utf-8

# In[ ]:


##构建测试数据,三个输入，范围-1，1，三个输出，三分类问题
#当x1>=0.55,x2<=-0.45时输出[1 0 0]',当x2>-0.05,x3>0.1时输出[0 1 0]',其他情况为[0 0 1]'
import numpy as np
import matplotlib.pyplot as plt
#测试数据个数m
m=1000
a1=2*np.random.random((3,m))-1
y=np.zeros((3,m))
for i in range(m):
    if a1[0,i]>=0.05 and a1[1,i]<=-0.15:
        y[0,i]=1
    elif a1[1,i]>=-0.05 and a1[2,i]>=0.1:
        y[1,i]=1
    else:
        y[2,i]=1
#print(y)
ba1=np.concatenate((np.ones((1,m)),a1),0)
#print(pa1)
#有一个隐藏层，有5个隐藏层单元
theta1=np.random.random((3,5))/100
btheta1=np.concatenate((np.ones((1,5)),theta),0)
h=lambda Z:1/(1+np.exp(-Z))
a2=h(btheta1.T.dot(ba1))
ba2=np.concatenate((np.ones((1,m)),a2),0)
theta2=np.random.random((5,3))/100
btheta2=np.concatenate((np.ones((1,3)),theta2),0)
y1=h(btheta2.T.dot(ba2))
#代价函数J,正则化系数k
k=1
#J=-1/m*np.sum(y*np.log(y1)+(1-y)*np.log(y1))+1/(2*m)*(np.sum(k*theta1)+np.sum(k*theta2))
#计算偏差

iteration=1000
J=np.zeros((1,iteration))
for j in range(iteration):
    J[0,j]=-1/m*np.sum(y*np.log(y1)+(1-y)*np.log(y1))+1/(2*m)*(np.sum(k*theta1)+np.sum(k*theta2))
    delta3=y1-y
    bdelta3=delta3
    delta2=theta2.dot(delta3)*a2*(1-a2)
    bdelta2=btheta2.dot(delta3)*ba2*(1-ba2)
    #delta1=theta1.dot(delta2)*a1*(1-a1)
    #bdelta1=btheta1.dot(delta2)*ba1*(1-ba1)
    #计算梯度下降算法所需要的偏导数
    #print(ba2.shape)
    #print(bdelta3.shape)
    bD2=1/m*ba2.dot(delta3.T)
    bD1=1/m*ba1.dot(delta2.T)
    #print(bD1.shape)
    #print(bD2.shape)
    bD1[0,:]=0
    bD2[0,:]=0
    change1=k*btheta1
    change1[1:4,:]=0
    change2=k*btheta2
    change2[1:6,:]=0
    bD1=bD1+change1
    bD2=bD2+change2
    #更新theta，学习率为0.1
    alpha=0.01
    theta1=theta1-alpha*bD1[1:4,:]
    theta2=theta2-alpha*bD2[1:6,:]
    btheta1=btheta1-alpha*bD1
    btheta2=btheta2-alpha*bD2
yy=np.arange(iteration)+1
yy=yy[np.newaxis,:]
#print(yy)
#print(J)
plt.scatter(yy,J)
plt.plot()
#print(btheta1)
#print(btheta2)
#training data:0.2,0.3,0.5
train_a1=np.array([[0.2],[0.3],[0.5]])
#预计输出应该是[0 1 0]
train_ba1=np.concatenate((np.ones((1,1)),train_a1),0)
train_a2=h(btheta1.T.dot(train_ba1))
train_ba2=np.concatenate((np.ones((1,1)),train_a2),0)
train_y1=h(btheta2.T.dot(train_ba2))
print(train_y1)
#print(y)


# In[ ]:





# In[ ]:




